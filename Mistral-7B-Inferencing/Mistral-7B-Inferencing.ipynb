{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHDnWyp5GJVd"
      },
      "source": [
        "##  Inferencing on Mistral 7B LLM with 4-bit quantization ğŸš€ - In FREE Google Colab\n",
        "\n",
        "# [Link to my Youtube Video Explaining this whole Notebook](https://www.youtube.com/watch?v=eovBbABk3hw&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=10&ab_channel=Rohan-Paul-AI)\n",
        "\n",
        "[![Imgur](https://imgur.com/Lz4ov4K.png)](https://www.youtube.com/watch?v=eovBbABk3hw&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=10&ab_channel=Rohan-Paul-AI)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WqidwNaaGJVh",
        "outputId": "4adb5e2b-f51a-4626-ea9c-e4e55ef090ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers -q peft  accelerate bitsandbytes safetensors sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVqy1hXqGJVj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = 'bn22/Mistral-7B-Instruct-v0.1-sharded'\n",
        "\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    :param model_name: Name or path of the model to be loaded.\n",
        "    :return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Initialize the tokenizer with the specified model_name.\n",
        "\n",
        "    :param model_name: Name or path of the model for tokenizer initialization.\n",
        "    :return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "model = load_quantized_model(model_name)\n",
        "\n",
        "tokenizer = initialize_tokenizer(model_name)\n",
        "\n",
        "# Define stop token ids\n",
        "stop_token_ids = [0]\n",
        "\n",
        "text = \"[INST] How AI will replace Engineers [/INST]\"\n",
        "\n",
        "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_input = encoded\n",
        "generated_ids = model.generate(**model_input, max_new_tokens=200, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "O texto a seguir Ã© uma transcriÃ§Ã£o de atendimento:\n",
        "\n",
        "Atendente: Obrigado por ligar para o ServiÃ§o de Atendimento ao Cliente da ABC EletrÃ´nicos. Meu nome Ã© Maria. Como posso ajudar vocÃª hoje?\n",
        "Cliente: OlÃ¡, Maria. Meu nome Ã© JoÃ£o, e tenho enfrentado muitos problemas com o produto que comprei da sua empresa. Comprei um smartphone modelo ABC123 hÃ¡ algumas semanas, e ele sÃ³ tem me dado problemas.\n",
        "Atendente: Lamento ouvir isso, JoÃ£o. Entendo como isso pode ser frustrante. VocÃª poderia descrever os problemas que estÃ¡ enfrentando com o seu smartphone ABC123?\n",
        "Cliente: Bem, a vida Ãºtil da bateria Ã© terrÃ­vel, e o aparelho reinicia aleatoriamente. A qualidade da cÃ¢mera nÃ£o corresponde ao que foi anunciado, e ele trava Ã s vezes. Estou realmente decepcionado com essa compra.\n",
        "Atendente: PeÃ§o desculpas pela inconveniÃªncia que vocÃª estÃ¡ enfrentando. Levamos essas questÃµes a sÃ©rio, e gostaria de ajudar a resolver esses problemas. Vamos comeÃ§ar verificando algumas coisas. VocÃª tentou reiniciar o telefone ou realizar uma restauraÃ§Ã£o de fÃ¡brica?\n",
        "Cliente: Sim, tentei essas etapas, e os problemas persistem. Atualizei o software tambÃ©m, mas nÃ£o resolveu.\n",
        "Atendente: Obrigada por tentar essas etapas, JoÃ£o. Como o seu telefone ainda estÃ¡ na garantia, podemos enviar a vocÃª um aparelho de substituiÃ§Ã£o ou agendar um tÃ©cnico para avaliar e reparar o seu telefone atual. Qual opÃ§Ã£o vocÃª prefere?\n",
        "Cliente: Acho que gostaria de optar pela substituiÃ§Ã£o, Maria. Tive muitos problemas com esse telefone, e preciso de um aparelho confiÃ¡vel.\n",
        "Atendente: Entendi, JoÃ£o. Vou processar o pedido de substituiÃ§Ã£o para vocÃª. Deve chegar nos prÃ³ximos dias Ãºteis. TambÃ©m vou enviar a vocÃª os detalhes de envio e as instruÃ§Ãµes para devolver o seu telefone atual por e-mail.\n",
        "Cliente: Obrigado, Maria. AgradeÃ§o sua ajuda na resoluÃ§Ã£o deste problema. Espero que o aparelho de substituiÃ§Ã£o funcione melhor.\n",
        "Atendente: De nada, JoÃ£o. Estamos aqui para garantir a satisfaÃ§Ã£o dos nossos clientes. Se vocÃª tiver algum problema com a substituiÃ§Ã£o ou tiver outras preocupaÃ§Ãµes, nÃ£o hesite em nos contatar. HÃ¡ mais alguma coisa em que posso ajudar hoje?\n",
        "Cliente: NÃ£o, Ã© sÃ³ isso por enquanto. Obrigado pela sua assistÃªncia, Maria.\n",
        "Atendente: Foi um prazer, JoÃ£o. Tenha um Ã³timo dia, e esperamos que o novo telefone atenda Ã s suas expectativas.\n",
        "\n",
        "Identifique as reclamaÃ§Ãµes do cliente.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "b38_r1VNHvE5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_input = encoded\n",
        "generated_ids = model.generate(**model_input, max_new_tokens=400, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "AJvwOI9nG0wo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}